#!/usr/bin/env python3

# -*- coding: utf-8 -*-
"""
Finds ThorSync directories corresponding to ThorImage directories
by looking for filenames or a descriptions of pins used in each
presentation, and matching that to the pins the Arduino says it 
uses, decoded from the ThorSync data.

Need to have pin signalled as in my OlfStimDelivery.ino script and 
follow my practice of copying something containing either the 
filename or the full pins to be presented in to the Notes field
in the ThorImage Capture settings.

Created on Wed Mar 15 15:02:09 2017

@author: Tom O'Connell
"""

# TODO refactor this script into one that just matches thorsync directories
# and one that figures out stimulus stuff

from __future__ import print_function

import os
import xml.etree.ElementTree as etree
import ast
import re
import pickle
import datetime

import h5py
import numpy as np

#from .. import tom.analysis as ta
import tom.analysis as ta
import tom.odors as odors

# TODO fix
print('WARNING: you currently have to run this script from the al_imaging directory ' + \
        'in order for it to be able to import some stuff.')

# TODO move below functions into shared util between this and analysis

def get_fly_id(directory_name):
    return '_'.join(os.path.split(directory_name)[-1].split('_')[:2])


def get_session_id(directory_name):
    return '_'.join(os.path.split(directory_name)[-1].split('_')[2])


def get_exptime(thorimage_dir):
    expxml = os.path.join(thorimage_dir, 'Experiment.xml')
    return int(etree.parse(expxml).getroot().find('Date').attrib['uTime'])


def get_synctime(thorsync_dir, time=os.path.getmtime):
    syncxml = os.path.join(thorsync_dir, 'ThorRealTimeDataSettings.xml')
    return time(syncxml)


def get_readable_exptime(thorimage_dir):
    expxml = os.path.join(thorimage_dir, 'Experiment.xml')
    return etree.parse(expxml).getroot().find('Date').attrib['date']


def readable_timestamp(unix_timestamp):
    return datetime.datetime.fromtimestamp(unix_timestamp)


# warn if has SyncData in name but fails this?
def is_thorsync_dir(d):
    if not os.path.isdir(d):
        return False
    
    files = {f for f in os.listdir(d)}

    have_settings = False
    have_h5 = False
    for f in files:
        # checking for substring
        if 'ThorRealTimeDataSettings.xml' in f:
            have_settings = True
        if '.h5':
            have_h5 = True
            
    return have_h5 and have_settings


# TODO actually check how OME tifs are indexed?
def is_anatomical_stack(d):
    return 'anat' in d


def read_old_pin2odor(picklefile):
    with open(picklefile, 'rb') as f:
        pin2odor_list = pickle.load(f)

    # TODO convert to tuple?
    return [[(t[0], odors.str2pair(t[1]), t[2]) for t in l] for l in pin2odor_list]


make_changes = False
# TODO remove this option?
ignore_thorsync = True

# in here should be a directory called 'stimuli'
# which should have pickle files generated by *_odor_randomizer.py

# TODO use environment variables / config file
'''
stimuli_base_dir = 'D:/Hong/Tom/stimuli'
thorimage_dir = 'D:/Hong/Tom/flies'
thorsync_dir = 'D:/Hong/Tom/flies'
'''
stimuli_base_dir = '/media/threeA/Tom/stimuli'
thorimage_dir = '/media/threeA/Tom/flies'
thorsync_dir = '/media/threeA/Tom/flies'

sync_dirs = [os.path.join(thorsync_dir,d) for d in os.listdir(thorsync_dir)]
sync_dirs = sorted([d for d in sync_dirs if is_thorsync_dir(d)], key=get_synctime)

#ordered_sync_dirs = [x for x,y in sorted([(d, get_synctime(d)) \
#        for d in sync_dirs], key=lambda x: x[1])]

valid_fly_ids = re.compile(r'(\d{6}_\d{2}(?:e|c)?_)')

image_dirs = [os.path.join(thorimage_dir,d) for d in os.listdir(thorimage_dir)]
image_dirs =  [d for d in image_dirs \
        if os.path.isdir(d) and valid_fly_ids.search(d)]

# TODO flag to override assertion errors or to ask y/n instead
# TODO add colors to warnings and green if things pass

# doesn't actually check for the ../../stimuli/ prefix
# assumes in is in stimuli_base_dir above
stimfile_pattern = re.compile(r'(\d{4}-\d{2}-\d{2}_\d{6}\.p)')

# are angle / square brackets special chars?
#pinlist_re = re.compile(r'\[\{(?:\d\d?(?:, )?)+\}(?:, )?\]+')
pinlist_re = re.compile(r'\[(?:\[(?:\{(?:\d\d?(?:, )?)+\}(?:, )?)+\](?:, )?)+\]')
pin_odor_port_re = re.compile(r'(\d\d?) -> (.* 1e-?\d\d?) -> ([A-Z])')

fly2pl = dict()
fly2ac = dict()
fly2stim = dict()

fly_ids = []

stimfile2imgdir = dict()
without_stimfile = []

without_thorsync = []

sync_image_mismatch_cutoff = 2 # seconds

# TODO specifically check that all things with same session ID
# seem to be using the same stimulus panel

for img_dir in image_dirs:
    print(img_dir)
    fly_id = get_fly_id(img_dir)
    
    if is_anatomical_stack(img_dir):
        continue
    fly_ids.append(fly_id)

    can_attempt_decode = False

    if not ignore_thorsync:
        in_img_dir = [os.path.join(img_dir,o) for o in os.listdir(img_dir)]
        possible_thorsync_dirs = [d for d in in_img_dir if is_thorsync_dir(d)]

        if len(possible_thorsync_dirs) > 1:
            raise Exception('found too many possible thorsync directories in ' + \
                str(img_dir) + '. should only ever be at most one.')

        elif len(possible_thorsync_dirs) == 1:
            # (in seconds)
            sync_exp_timediff = get_synctime(possible_thorsync_dirs[0]) - get_exptime(img_dir)
            assert abs(sync_exp_timediff) < sync_image_mismatch_cutoff, \
                'nested ThorSync XML file (in ' + \
                possible_thorsync_dirs[0] + ') modified ' + str(sync_exp_timediff) + \
                ' seconds apart from date in ThorImage Experiment.xml'

            # TODO may have to reference and handle pickle files of old format for these directories
            # TODO should only skip if also have generated stimulus metadata
            can_attempt_decode = True

            # TODO put this back if refactor
            #print('skipping ' + str(img_dir) + ' because found nested ThorSync directory')
            # skip this directory, because it already seems matched
            #continue
        
        else:
            without_thorsync.append(img_dir)

    expxml = os.path.join(img_dir, 'Experiment.xml')
    notes = etree.parse(expxml).getroot().find('ExperimentNotes').attrib['text']
    stimfile = stimfile_pattern.search(notes)

    # TODO warn if more than one found?
    
    if stimfile is not None:
        stimfile = stimfile.group(0)
        unplaced_stimfile = os.path.join(stimuli_base_dir, stimfile)
        
        if not os.path.isfile(unplaced_stimfile):
            placed_stimfile = os.path.join(img_dir, stimfile)
            if os.path.isfile(placed_stimfile):
                stimfile = placed_stimfile
            else:
                print(stimfile + ' went missing!')
                stimfile = None
        else:
            # TODO so can i actually load placed stimfiles?
            stimfile = unplaced_stimfile
            
    if stimfile is not None:
        stimfile2imgdir[stimfile] = img_dir

        # TODO handle the condition where i have stimfiles that hold a different pickle
        # (just acs, probably in same format)
        # may want to manually move those stimfiles to position prior
        
        with open(stimfile, 'rb') as f:
            acs_raw, odor_list_raw = pickle.load(f)
            acs_loaded = tuple(tuple(sorted(l, key=lambda x: x[0])) for l in acs_raw)
            odor2pin_list = [{trip[1]: trip[0] for trip in acs} for acs in acs_loaded]
            # TODO convert odorset list to pinset list
            pl_loaded = tuple(tuple(set(d[o] for o in s) for s in l) \
                              for l, d in zip(odor_list_raw, odor2pin_list))

    # TODO need to not give a false sense of security by
    # saying that this generated stimulus list (decoded from ThorSync data)
    # matches the stimuli decoded from the ThorSync data in analysis
    elif can_attempt_decode:
        # TODO check to see if i tend to reference the file in ../../pin2odor
        print(notes)

        real_pins, _ = ta.decode_from_session_directory(d)
        # TODO is this the same format as the others?
        fly2pl[fly_id] = real_pins
            
    else:
        acs_loaded = None
        pl_loaded = None
        
        without_stimfile.append(img_dir)
        
    # TODO check loaded stuff is consistent with everything else (including order?),
    # and if it is, add it to same dict. directly comparable? tuple?
        
    pl_match = pinlist_re.search(notes)
    pl = None
    
    # TODO check loaded information with parsed information
    if pl_match is not None:
        #if pl_loaded is None:
        #    print('order')
        pl = ast.literal_eval(pl_match.group(0))
        pl = tuple(tuple(l) for l in pl)
        
        # IF WE HAVE DATA FOR THIS SPECIFIC SESSION, this checks that it is 
        # apparently consistent with information from flies with the same id
        if fly_id in fly2pl:
            assert pl == fly2pl[fly_id], 'apparently conflicting list of pins' + \
                ' presented across flies with the same id: ' + fly_id
        else:
            fly2pl[fly_id] = pl
            
        # checks consistency with pin order loaded from pickle file
        # currently will only work if full pin order can be parsed from
        # notes in each session
        if pl_loaded is not None:
            assert pl == pl_loaded, 'pin order loaded from stimulus pickle ' + \
                'file is apparently inconsistent with pin order in notes. ' + \
                fly_id
    
    #elif pl_loaded is not None:
    if pl_loaded is not None:
        # check might be unnecessary
        if fly_id in fly2pl:
            assert pl_loaded == fly2pl[fly_id], 'apparently conflicting ' + \
                'list of pins presented across flies with ' + \
                'the same id (one of which was loaded from pickle): ' + fly_id
        else:
            fly2pl[fly_id] = pl_loaded

    # TODO can now test all for consistency with thorsync (though may be overkill...
    # i do that in analysis now, and most things so far have matched)
    
    # assumes connections will be printed out in a consistent order
    # across all files compared here (to check equality of parsed tuples)
    # also assumes that if there are multiple sets of connections printed, 
    # they are separated by the word 'stoppers' somewhere
    
    # will only use first 3.
    all_connections = [pin_odor_port_re.findall(n) for n in notes.split('stoppers')[:3]]
    all_connections = tuple(tuple((int(x[0]), odors.str2pair(x[1]), x[2]) for x in ac) \
                            for ac in all_connections if len(ac) > 0)
    
    # TODO careful. this may break easily
    if len(all_connections) > 1:
        #if acs_loaded is None:
        #    print('connections')

        # IF WE HAVE DATA FOR THIS SPECIFIC SESSION, this checks that it is 
        # apparently consistent with information from flies with the same id
        if fly_id in fly2ac:
            assert all_connections == fly2ac[fly_id], 'apparently conflicting ' + \
                'pin (valve) to odor connections presented across flies with ' + \
                'the same id: ' + fly_id
        else:
            fly2ac[fly_id] = all_connections
            
        # TODO test this section and the below!
        if acs_loaded is not None:
            assert all_connections == acs_loaded, 'connections described in notes ' + \
                'are apparently inconsistent with those in stimulus pickle file ' + \
                fly_id + '\n\nnotes:' + str(all_connections) + '\n\nloaded:' + str(acs_loaded)
                
    # TODO maybe clean this section up? might be redundant
    if acs_loaded is not None:
        # check might be unnecessary
        if fly_id in fly2ac:
            assert acs_loaded == fly2ac[fly_id], 'apparently conflicting ' + \
                'pin (valve) to odor connections presented across flies with ' + \
                'the same id (one of which was loaded from pickle): ' + fly_id
        else:
            fly2ac[fly_id] = acs_loaded

fly_ids = set(fly_ids)
#print('Processing fly IDs:')
#print(fly_ids)


#############################################################################################
# check for possible red flags across flies (like exact same stimulus metadata)
#############################################################################################

# TODO warn and include list of things we dont have stimulus for at this point
# mappings and pins separately?
    
# could happen by chance with reasonable probability if you have a really
# small odor panel

# TODO write unit tests for these!!!

# TODO but this will happen if something is run twice
# maybe just delete in advance trials that went badly?
# checks the same pin order isn't observed twice in recording
# sessions with different fly_ids (even though not all recordings
# list this)
# would likely indicate a data entry error
for fly in fly2pl:
    copy = dict(fly2pl)
    del copy[fly]
    assert fly2pl.values() != copy.values(), 'possible duplication data entry ' + \
        'error (' + fly + ' and at least one other are involved)'

# checks the same connection pattern isn't observed twice in recording
# sessions with different fly_ids. same reason.
for fly in fly2ac:
    copy = dict(fly2ac)
    del copy[fly]
    assert set(fly2ac.values()) != set(copy.values()), \
        'possible duplication data entry error (' + fly + ' and at least one other are involved)'

assert len(stimfile2imgdir.keys()) == len(stimfile2imgdir.values()), \
    'this should not happen, but dont want to do something dangerous'

#############################################################################################

'''
# if we have not failed any checks so far, move the stimulus files 
# to their matching ThorImage directories
for sf in stimfile2imgdir:
    dst = stimfile2imgdir[sf]
    assert os.path.isdir(dst)
    # should copy (some?) operating system metadata too
    # TODO prompt first?
    shutil.copy2(sf, dst)
'''

# TODO only print the ones nothing can be found for !!!
# wasn't i doing that before?

# generate correct pin 2 odor mappings from notes in available
# save in ThorImage directory as differently named pickle file
# to be read by analysis software
print('Valve to odor connections and stimulus order pickle file not found for:')
for d in without_stimfile:
    print(d)
print('')

print('Have stimulus order (without decoding) for:')
for f in fly2pl:
    print(f)
    assert not (fly2pl[f] is None or (type(fly2pl[f]) != tuple and type(fly2pl[f]) != list)), \
            'bad fly2pl entry' + str(fly2pl[f])

print('Would need to decode for stimulus order for:')
for f in fly_ids:
    if f not in fly2pl:
        print(f)


#############################################################################################
# store the ordering of sessions, to match up to correct index of stimulus metadata list
#############################################################################################

dir2session_idx = dict()

for fid in fly_ids:
    # TODO sorted in correct order?
    fid_indexed_dirs = [(i,d) for i,d in \
        enumerate(sorted(image_dirs, key=get_exptime)) \
            if fid in d and not is_anatomical_stack(d)]
    
    # should all be consecutive
    # we don't take flies off and put the same ones back on later
    # after running other flies
    dir_indices = [i for i,d in fid_indexed_dirs]
    min_idx = min(dir_indices)
    max_idx = max(dir_indices)
    # TODO test
    for i in range(min_idx, max_idx + 1):
        assert i in dir_indices, 'directories for fly ' + fid + \
            ' are apparently not consecutive (by date in Experiment.xml)'
    
    fid_dirs = [d for i,d in fid_indexed_dirs]
    idx = 0
    last_session = None
    previous_sessions = set()
    for d in fid_dirs:
        session = get_session_id(d)
        
        if last_session is not None and last_session != session:
            idx += 1
            
            # if the new session has already been seen before,
            # it would suggest something fishy
            assert not session in previous_sessions, 'repeats of ' + \
                'session ' + session + ' apparently not consecutive'
            
                
        dir2session_idx[d] = idx
        
        last_session = session
        previous_sessions.add(session)


#############################################################################################
# save stimulus metadata in a new format, to be read by analysis
#############################################################################################


for d in image_dirs:
    # maybe handle this once earlier in manner that avoids checking repeatedly?
    if is_anatomical_stack(d):
        continue
    
    fly_id = get_fly_id(d)

    # TODO just prevent it from being added as None
    if fly_id in fly2ac and fly2ac[fly_id] is not None:
        gen_connections = os.path.join(d, 'generated_pin2odor.p')
        
        if make_changes:
            print('saving valve to odor connections for ' + \
                  fly_id + ' to ' + gen_connections)
        
            with open(gen_connections, 'wb') as f:
                # TODO prompt first?           
                pickle.dump(fly2ac[fly_id][dir2session_idx[d]], f)

    if fly_id in fly2pl and fly2pl[fly_id] is not None:
        gen_connections = os.path.join(d, 'generated_stimorder.p')
        
        if make_changes:
            print('saving list of pins switched on each trial for ' + \
                  fly_id + ' to ' + gen_connections)
        
            with open(gen_connections, 'wb') as f:
                # TODO prompt first?           
                pickle.dump(fly2pl[fly_id][dir2session_idx[d]], f)


#############################################################################################
# move ThorSync directories from their home in thorsync_dir to the matching directory
# in thorimage_dir
#############################################################################################

if not ignore_thorsync:
    stamped_sync_dirs = [(get_synctime(s), s) for s in sync_dirs]

    to_move = dict()

    if not make_changes:
        print('data directories without ThorSync subdirectories:')

    # TODO include anat as well
    for d in sorted(without_thorsync, key=get_exptime):
        nearest = min(stamped_sync_dirs, key=lambda x: abs(x[0] - get_exptime(d)))
        nearest_thorsync= nearest[1]
            
        if not make_changes:
            print(d)
            print(nearest[0] - get_exptime(d))
            print(nearest_thorsync)
            print('')
        
        '''
        if abs(nearest[0] - get_exptime(d)) > 1:
            print(d)
            print(nearest[0] - get_exptime(d))
            print(nearest_thorsync)
            print('')
        '''
            
        if abs(nearest[0] - get_exptime(d)) < sync_image_mismatch_cutoff:
            to = os.path.join(d, os.path.split(nearest_thorsync)[-1])
            if make_changes:
                os.rename(nearest_thorsync, to)
            to_move[nearest_thorsync] = to
          
    # TODO get rid of common prefix. include shebang line.
    for src in to_move:
        # the -n flag prevents overwriting, the -v makes it verbose
        print('mv -nv', src, to_move[src])

"""
for d in sync_dirs:
    # ctime and mtime don't always agree, but almost always do
    '''
    print(get_synctime(d))
    print(get_synctime(d, time=os.path.getctime))
    print(np.isclose(get_synctime(d), get_synctime(d, time=os.path.getctime)))
    '''
    print(readable_timestamp(get_synctime(d)), d)
"""
    
# can mostly ignore this for now because matching SyncData with the anat
# is less important
# and can just move everything else to another directory for now
'''
# clean up ThorSync directories from trials that have no frames
# other indications of a bad run? # indices / sampling rate?
# if it was stopped while frame / something else was still changing?
# / while acquisition trigger was high?
for d in sync_dirs:
    # TODO what was up with directories i saw with multiple h5 files?
    h = h5py.File(os.path.join(d,'Episode001.h5'), 'r')
    # should be the max?
    # TODO why was this not always nested in another (list?)
    print(h['CI']['frame_counter'][-1])
    #max_frame = np.array(h['CI']['frame_counter']).max()
    #print(max_frame)

print(len(sync_dirs))
print(len(without_thorsync))
'''

# for each pickle file, find the ThorSync file that matches that 
# pin presentation order after decoding

# TODO make sure no two SyncData files decode to the same thing
# ALTHOUGH THIS COULD HAPPEN... MAYBE DONT
# TODO if >1 SyncData decode to same thing (and loaded parsed data does too)
# match them up in time and warn
# move matching to imaging directories

# if any imaging directories do not appear to have corresponding
# ThorSync data, there is a serious problem that the user needs
# to address

# move non-matching to orphans
